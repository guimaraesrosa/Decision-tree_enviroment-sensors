library(lattice)
library(rpart)
library(ggplot2)
library(caret)
library(rattle)					# Fancy tree plot
library(rpart.plot)			# Enhanced tree plots
library(RColorBrewer)		# Color selection for fancy tree plot
library(party)					# Alternative decision tree algorithm
library(AUC)
library(pROC)
library(plotROC)
library(zoo)

#Reading from Database
dados = read.csv("Base-15min-Rosana-Categorizada.csv",sep=",", header=T)
head(dados)

#Sum Class
summary(factor(dados$classes))

## 70% of the sample size
sample_size <- floor(0.7 * nrow(dados))

## set the seed to make your partition reproductible
set.seed(123422)
train_index <- sample(seq_len(nrow(dados)), size = sample_size)

x_train <- dados[train_index, ]
x_test <- dados[-train_index, ]

y_train <- x_train$classes
y_test  <- x_test$classes

x_train$classes<-NULL
x_test$classes<-NULL

# Create a stratified sample for repeated cv
#times: the number of partitions to create
#k: an integer for the number of folds.
#y_train: a vector of outcomes.
cv_10_folds<-createMultiFolds(y_train,k=10,times=10)

# create a control object for repeated cv in caret
#method: resampling method - "cv", "repeatedcv", "LGOCV" (for repeated training/test splits), "none" (only fits one model to the entire training set)
#number: Either the number of folds or number of resampling iterations
#repeats: For repeated k-fold cross-validation only: the number of complete sets of folds to compute
#index: a list with elements for each resampling iteration. Each list element is a vector of integers corresponding to the rows used for training at that iteration.
ctrl <- trainControl(method="repeatedcv",number=10,repeats=3,index=cv_10_folds)

#    CART MODEL
#trControl: A list of values that define how this function acts
#tuneLength: An integer denoting the amount of granularity in the tuning parameter grid. 
#By default, this argument is the number of levels for each tuning parameters that should be generated by train.
tree <-train(x=x_train,y=y_train,method="rpart",trControl=ctrl,tuneLength=5, cp=0.1)

y_predicted_tree<-predict(tree,x_test)
df<-data.frame(Orig=y_test,Pred=y_predicted_tree)
confusionMatrix(table(df$Orig,df$Pred))
plot(tree)
fancyRpartPlot(tree$finalModel)

#    RANDOM FOREST
rf <-train(x=x_train,y=y_train,method="rf",trControl=ctrl)

y_predicted_rf<-predict(rf,x_test)
df<-data.frame(Orig=y_test,Pred=y_predicted_rf)
confusionMatrix(table(df$Orig,df$Pred))
plot(rf)
# Variáveis mais significantes para o modelo
VarImportance <- varImp(rf)
# Plot the top 15 predictors
plot(VarImportance, main = "Variáveis mais significantes para o modelo Random Forest", top = 7)


#     BOOSTED TREE
boosted <-train(x=x_train,y=y_train,method="gbm",trControl=ctrl,tuneLength=5)

y_predicted_bt<-predict(boosted,x_test)
df<-data.frame(Orig=y_test,Pred=y_predicted_bt)
confusionMatrix(table(df$Orig,df$Pred))
plot(boosted, main="Boosted")

# Acurácia ##################
Model <- c("CART", "Random Forest","Boosted Tree")
Accuracy <- c(round(max(tree$results$Accuracy),4)*100,round(max(rf$results$Accuracy),4)*100,
              round(max(boosted$results$Accuracy),4)*100 )
Performance <- cbind(Model,Accuracy); 
Performance

#ROC curve - AUC
set.seed(1000)
# 3-class in response variable
#rf = randomForest(Species~., data = iris, ntree = 100)
# predict(.., type = 'prob') returns a probability matrix
#plot.roc or line.roc to visualize all roc curve to each classes
predictions <- as.numeric(predict(tree, x_test, type = 'raw'))
roc.multi <- multiclass.roc(y_test, predictions)
#auc(roc.multi)
rs <- roc.multi[['rocs']]
plot.roc(rs[[1]], main = "Curva ROC", legend = TRUE)
sapply(2:length(rs),function(i) lines.roc(rs[[i]],col=i,legend=TRUE))
legend("topright", c("Agradavel", "Razoavel", "Desagradavel"), lty=1, 
       col = c("black", "red","green"), bty="n", inset=c(0,0.30))

roc.multi <- multiclass.roc(y_test, predictions)
auc(roc.multi)
